{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gJh5bnLAL26"
      },
      "source": [
        "Link to Colab File : https://colab.research.google.com/drive/1kqVBkbwWwI9tRVGm5qXdcCgk--FYgwh7?usp=sharing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rbxu20AY7Opu"
      },
      "source": [
        "import numpy.random as npr\n",
        "import jax\n",
        "import numpy as np\n",
        "from jax import jit, grad\n",
        "import jax.numpy as jnp\n",
        "from sklearn.datasets import load_boston\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import KFold\n",
        "from functools import partial\n",
        "from jax.scipy.special import logsumexp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NOWjNuDAMqGq"
      },
      "source": [
        "class NeuralNetwork():\n",
        "  def __init__(self, layers_sizes, layers_activation, type_, frac = 0.01):\n",
        "    \"\"\"\n",
        "    Parameters :\n",
        "      > layers_sizes : Python List. Number of neurons in each layer.\n",
        "      > layers_activation : Python List. Type of activation for each hidden layer.\n",
        "                          Avaliable Options are {sigmoid, relu, identity}\n",
        "      > type_ : int. 0 for Regression, 1 for Classification.\n",
        "      > fac : int. Multiplicative factor for weights.\n",
        "    \"\"\"\n",
        "    self.layers_sizes = layers_sizes.copy()\n",
        "    self.layers_activation = layers_activation.copy()\n",
        "    self.num_layers = len(layers_sizes)\n",
        "    self.type_ = type_\n",
        "    self.frac = frac\n",
        "  \n",
        "  def __initialise_params(self):\n",
        "    \"\"\"\n",
        "    This function randomly initialises the weights for each layer.\n",
        "    \"\"\"\n",
        "    layer_sizes = self.layers_sizes\n",
        "    parameters = []\n",
        "    for i in range(1,len(layer_sizes)):\n",
        "      Wi = self.frac*npr.randn(layer_sizes[i],layer_sizes[i-1])\n",
        "      bi = self.frac*npr.randn(layer_sizes[i])\n",
        "      parameters.append([Wi,bi])\n",
        "    return parameters\n",
        "  \n",
        "  def __GetBatches(self, batch_size, X, y):\n",
        "    \"\"\"\n",
        "     Parameters :\n",
        "      > batch_size : int.\n",
        "      > X : numpy array.\n",
        "      > y : numpy array.\n",
        "     Returns :\n",
        "      > batches : python list. Contains tuples of (x,y), each tuple is a batch.\n",
        "    \"\"\"\n",
        "    batches = []\n",
        "    num_batches = X.shape[0]//batch_size\n",
        "    if (X.shape[0]%batch_size!=0):\n",
        "      num_batches += 1\n",
        "    data = np.hstack((np.copy(X), np.copy(y)))\n",
        "    n = len(data)\n",
        "    for i in range(num_batches):\n",
        "      batch = data[i*(batch_size):min(n,(i+1)*batch_size),:]\n",
        "      X_t = batch[:,:-1]\n",
        "      y_t = batch[:,-1]\n",
        "      if self.type_ == 1:\n",
        "        # Creating One-Hot Respresentation incase of classification\n",
        "        y_t = self.__oneHot(y_t)\n",
        "      batches.append((X_t,y_t))\n",
        "    return batches\n",
        "  \n",
        "  def __oneHot(self,y,num_classes = 10):\n",
        "    \"\"\"\n",
        "    Parameters :\n",
        "      > y : numpy array.\n",
        "      > num_classes : int. The number of classes that y can take.\n",
        "\n",
        "    Returns :\n",
        "      > One Hot Encoded respresentation of y, shape = (y.shape[0],num_classes). \n",
        "    \"\"\"\n",
        "    one_hot = np.zeros((len(y), num_classes))\n",
        "    for i in range(len(y)):\n",
        "      one_hot[i][int(y[i])] = 1\n",
        "    return one_hot\n",
        "\n",
        "  def __relu(self, x):\n",
        "    \"\"\"\n",
        "    Parameters :\n",
        "      > x : numpy array.\n",
        "    Returns :\n",
        "      > numpy array of element wise relu of x.\n",
        "    \"\"\"\n",
        "    return jax.nn.relu(x)\n",
        "    \n",
        "  def __sigmoid(self, x):\n",
        "    \"\"\"\n",
        "    Parameters :\n",
        "      > x : numpy array.\n",
        "    Returns :\n",
        "      > numpy array of element wise sigmoid of x.\n",
        "    \"\"\"\n",
        "    return 1/(1+jnp.exp(-x))\n",
        "  \n",
        "  def __identity(self, x):\n",
        "    \"\"\"\n",
        "    Parameters :\n",
        "      > x : numpy array.\n",
        "    Returns :\n",
        "      > Same Numpy array.\n",
        "    \"\"\"\n",
        "    return x\n",
        "\n",
        "  def __ForwardPass(self, parameters, X_cur):\n",
        "    \"\"\"\n",
        "    Parameters :\n",
        "      > parameters : Python List. Weights of the model.\n",
        "      > X_cur : The input of the current batch.\n",
        "\n",
        "    Returns :\n",
        "      > The output of the final layer of the Network.\n",
        "        Incase of classification, log of softmax is returned.\n",
        "    \"\"\"\n",
        "    A = X_cur\n",
        "    activation_functions = self.layers_activation\n",
        "    i = 0\n",
        "    for W,b in parameters[:-1]:\n",
        "      Z = jnp.dot(A, W.T) + b\n",
        "      if activation_functions[i] == \"relu\":\n",
        "        A = self.__relu(Z)\n",
        "      elif activation_functions[i] == \"sigmoid\":\n",
        "        A = self.__sigmoid(Z)\n",
        "      else:\n",
        "        A = self.__identity(Z)\n",
        "      i += 1\n",
        "    W,b = parameters[-1]\n",
        "    A = jnp.dot(A, W.T) + b\n",
        "    if self.type_ == 1:\n",
        "      # Incase of classification log of softmax is returned\n",
        "      return A - logsumexp(A, axis=1, keepdims=True)\n",
        "    else:\n",
        "      # Incase of regression a 1D Array is returned.\n",
        "      return A.reshape(-1)\n",
        "  \n",
        "  def cost_reg(self, parameters, batch):\n",
        "    \"\"\"\n",
        "    Parameters :\n",
        "      > parameters : Python List. Weights of the model.\n",
        "      > batch : Python tuple. The X and y values for the current batch.\n",
        "    \n",
        "    Returns :\n",
        "      > Mean of Sum of squared errors\n",
        "    \n",
        "    The function is used to generate gradients in the regression case. \n",
        "    \"\"\"\n",
        "    X_cur, y_cur = batch\n",
        "    n = len(X_cur)\n",
        "    y_hat = self.__ForwardPass(parameters, X_cur)\n",
        "    return jnp.dot((y_cur-y_hat).T,(y_cur-y_hat))/n\n",
        "  \n",
        "  def cost_clas(self, parameters, batch):\n",
        "    \"\"\"\n",
        "    Parameters :\n",
        "      > parameters : Python List. Weights of the model.\n",
        "      > batch : Python tuple. The X and y values for the current batch.\n",
        "    \n",
        "    Returns :\n",
        "      > Mean of cross-entropy loss.\n",
        "    \n",
        "    The function is used to generate gradients in the classification case. \n",
        "    \"\"\"\n",
        "    X_cur, y_cur = batch\n",
        "    y_hat = self.__ForwardPass(parameters, X_cur)\n",
        "    return -jnp.mean(jnp.sum(y_hat*y_cur, axis=1))\n",
        "  \n",
        "  @partial(jit, static_argnums=(0,))\n",
        "  def __update(self, parameters, batch, lr):\n",
        "    \"\"\"\n",
        "    Parameter :\n",
        "      > parameters : Python List, Weights of the model.\n",
        "      > batch : Python tuple. The X and y values for the current batch.\n",
        "      > lr : float. Learning Rate.\n",
        "\n",
        "    This Function updates the gradient.\n",
        "    This function is jit compiled. Reference : https://github.com/google/jax/issues/1251\n",
        "    \"\"\"\n",
        "    if self.type_ == 0:\n",
        "      grads = grad(self.cost_reg)(parameters, batch)\n",
        "    else:\n",
        "      grads = grad(self.cost_clas)(parameters, batch)\n",
        "    for i in range(len(parameters)):\n",
        "      parameters[i][0] -= (lr * grads[i][0])\n",
        "      parameters[i][1] -= (lr * grads[i][1])\n",
        "    return parameters\n",
        "  \n",
        "  def fit(self, X, y, batch_size, epochs = 150, lr = 0.01, lr_type = \"constant\"):\n",
        "    \"\"\"\n",
        "    Parameters :\n",
        "      > X : numpy array. Training Data.\n",
        "      > y : numpy array. Training Data labels.\n",
        "      > batch_size : int. Number of batches.\n",
        "      > epochs : int. Number of epochs.\n",
        "      > lr : float.  Learning Rate.\n",
        "      > lr_type : string. if \"constant\" learning rate remains constant throughout the learning process\n",
        "                            elif \"inverse\" learning rate decreases inverly with the epochs.\n",
        "    \"\"\"\n",
        "    X = X.copy()\n",
        "    y = y.copy()\n",
        "    y = y.reshape(-1,1)\n",
        "    input_size = len(X[0])\n",
        "    parameters = self.__initialise_params()\n",
        "    lr_cur = lr\n",
        "    batches = self.__GetBatches(batch_size, X, y)\n",
        "    for epoch in range(epochs):\n",
        "      if (lr_type == \"inverse\"):\n",
        "        lr_cur = lr_cur/(epoch+1)\n",
        "      for batch in batches:\n",
        "        parameters = self.__update(parameters, batch, lr_cur)\n",
        "    self.parameters = parameters\n",
        "  \n",
        "  def predict(self, X):\n",
        "    \"\"\"\n",
        "    Parameters :\n",
        "      > X : Numpy Array.\n",
        "    \n",
        "    Returns :\n",
        "      > The ouput value or class depending on regression or classification respectivey. \n",
        "    \"\"\"\n",
        "    parameters = self.parameters\n",
        "    y_hat = self.__ForwardPass(parameters,X)\n",
        "    if self.type_ == 1:\n",
        "      # Classification, picking the class with the highest log value of probablity.\n",
        "      output = jnp.argmax(y_hat, axis = 1)\n",
        "      return output\n",
        "    else:\n",
        "      return y_hat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRrDCnPM2sj4"
      },
      "source": [
        "def rmse(y, y_hat):\n",
        "  \"\"\"\n",
        "  Parameters :\n",
        "    > y : Ground Truth.\n",
        "    > y_hat : Prediction.\n",
        "  \n",
        "  Returns :\n",
        "    > Root Mean squared error.\n",
        "  \"\"\"\n",
        "  n = len(y)\n",
        "  rmse = 0\n",
        "  for i in range(n):\n",
        "    rmse += pow(y_hat[i]-y[i],2)\n",
        "  rmse = pow(rmse/y.size,0.5)\n",
        "  return rmse\n",
        "\n",
        "def accuracy(y,y_hat):\n",
        "  \"\"\"\n",
        "  Parameters :\n",
        "    > y : Ground Truth.\n",
        "    > y_hat : Prediction.\n",
        "  Returns :\n",
        "  > Accuracy.\n",
        "  \"\"\"\n",
        "  acc = 0\n",
        "  for i in range(y.size):\n",
        "    if (y[i]==y_hat[i]):\n",
        "      acc+=1\n",
        "  return acc/y.size\n",
        "\n",
        "def precprocess(X):\n",
        "    \"\"\"\n",
        "    Parameters :\n",
        "    > X : numpy array.\n",
        "\n",
        "    Returns :\n",
        "    > Input numpy array row-wise flattened and all values normalised to [0-1)\n",
        "    \"\"\"\n",
        "    n = len(X)\n",
        "    return X.reshape((n,-1))/15"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrHv6EI52N3B",
        "outputId": "28aa944e-1cdb-4660-c52d-f10e39954c54"
      },
      "source": [
        "# K fold Cross Validation, Regression case on boston Dataset.\n",
        "\n",
        "a = [13, 64, 128, 128, 64, 1]\n",
        "b = [\"relu\",\"relu\",\"relu\",\"relu\"]\n",
        "data = load_boston()\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(data[\"data\"])\n",
        "X = scaler.transform(data[\"data\"])\n",
        "y = data[\"target\"]\n",
        "kf = KFold(3)\n",
        "kf.get_n_splits(X)\n",
        "err = 0\n",
        "for train_index, test_index in kf.split(X):\n",
        "  # Test-Train Split loop\n",
        "  X_train, y_train = X[train_index], y[train_index]\n",
        "  X_test, y_test = X[test_index], y[test_index]\n",
        "  NN = NeuralNetwork(layers_sizes = a, layers_activation = b, type_ = 0)\n",
        "  NN.fit(X_train,y_train,50, epochs = 1000, lr = 0.001)\n",
        "  y_hat = NN.predict(X_test)\n",
        "  err += rmse(y_test,y_hat)\n",
        "print(\"Average 3 Fold rmse : \", err/3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Average 3 Fold rmse :  4.8850346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8-lXXu7c8XWr",
        "outputId": "e8995807-9bbf-44d4-c0c2-3a7fde90d3c8"
      },
      "source": [
        "# K fold Cross Validation, Regression case on boston Dataset.\n",
        "\n",
        "a = [13, 64, 32, 16, 8, 1]\n",
        "b = [\"relu\",\"relu\",\"relu\",\"relu\",\"relu\"]\n",
        "data = load_boston()\n",
        "scaler = MinMaxScaler()\n",
        "scaler.fit(data[\"data\"])\n",
        "X = scaler.transform(data[\"data\"])\n",
        "y = data[\"target\"]\n",
        "kf = KFold(3)\n",
        "kf.get_n_splits(X)\n",
        "err = 0\n",
        "for train_index, test_index in kf.split(X):\n",
        "  # Test-Train Split loop\n",
        "  X_train, y_train = X[train_index], y[train_index]\n",
        "  X_test, y_test = X[test_index], y[test_index]\n",
        "  NN = NeuralNetwork(layers_sizes = a, layers_activation = b, type_ = 0)\n",
        "  NN.fit(X_train,y_train,50, epochs = 1000, lr = 0.001)\n",
        "  y_hat = NN.predict(X_test)\n",
        "  err += rmse(y_test,y_hat)\n",
        "print(\"Average 3 Fold rmse : \", err/3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average 3 Fold rmse :  5.34297\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnMMA4zsNgDS",
        "outputId": "42fd667c-d6b6-4898-93b6-d26d4cc7f7f5"
      },
      "source": [
        "# K fold Cross Validation, Classification case on digits Dataset.\n",
        "\n",
        "a = [64, 128, 128, 10]\n",
        "b = [\"sigmoid\",\"sigmoid\",\"sigmoid\"]\n",
        "data = load_digits()\n",
        "X = precprocess(data[\"data\"])\n",
        "y = data[\"target\"]\n",
        "kf = KFold(3)\n",
        "kf.get_n_splits(X)\n",
        "acc = 0\n",
        "for train_index, test_index in kf.split(X):\n",
        "  # Test-Train Split loop\n",
        "  X_train, y_train = X[train_index], y[train_index]\n",
        "  X_test, y_test = X[test_index], y[test_index]\n",
        "  NN = NeuralNetwork(layers_sizes = a, layers_activation = b, type_ = 1, frac = 1)\n",
        "  NN.fit(X_train,y_train, 50, epochs = 1000, lr = 0.02)\n",
        "  y_hat = NN.predict(X_test)\n",
        "  acc += accuracy(y_test, y_hat)\n",
        "print(\"Average 3 Fold acc : \", acc/3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average 3 Fold acc :  0.9220923761825265\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JmKPdeYs9Q_t",
        "outputId": "53435d5b-e584-41f1-b612-0d61d6fe998e"
      },
      "source": [
        "# K fold Cross Validation, Classification case on digits Dataset.\n",
        "\n",
        "a = [64, 128, 128, 64, 32, 10]\n",
        "b = [\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\",\"sigmoid\"]\n",
        "data = load_digits()\n",
        "X = precprocess(data[\"data\"])\n",
        "y = data[\"target\"]\n",
        "kf = KFold(3)\n",
        "kf.get_n_splits(X)\n",
        "acc = 0\n",
        "for train_index, test_index in kf.split(X):\n",
        "  # Test-Train Split loop\n",
        "  X_train, y_train = X[train_index], y[train_index]\n",
        "  X_test, y_test = X[test_index], y[test_index]\n",
        "  NN = NeuralNetwork(layers_sizes = a, layers_activation = b, type_ = 1, frac = 1)\n",
        "  NN.fit(X_train,y_train, 50, epochs = 1000, lr = 0.02)\n",
        "  y_hat = NN.predict(X_test)\n",
        "  acc += accuracy(y_test, y_hat)\n",
        "print(\"Average 3 Fold acc : \", acc/3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Average 3 Fold acc :  0.9276572064552031\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eulNVPI-Hg0w"
      },
      "source": [
        "#### Reference : \n",
        "Examples given in offical JAX Github Repository : https://github.com/google/jax/tree/master/examples"
      ]
    }
  ]
}